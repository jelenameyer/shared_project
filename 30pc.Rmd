---
title: "Heartdisease_alg_comparison"
author: "Jelena Meyer and Hansj√∂rg Neth"
date: "2023-11-15"
output: html_document
---

```{r}
require(stats)
require(FFTrees)
```


# standardise data, because some algorithms perform better with standardised data (to be fair for those)
```{r}
heartdisease <- as.data.frame(lapply(heartdisease, function(x) {
  if(is.numeric(x)) scale(x) else x
}))
```



# divide data into 50% train and test data:
```{r}

nrow(heartdisease)

# set seed for reproducability
set.seed(13)

# take 50% of rows of df and build fitting df
heart_fit_indices <- sample(nrow(heartdisease), 0.5 * nrow(heartdisease), replace = FALSE)
heart_fit <- heartdisease[heart_fit_indices,]
nrow(heart_fit)


# find remaining rows and put into predicting df
heart_predict_indices <- setdiff(1:nrow(heartdisease), heart_fit_indices)
heart_predict <- heartdisease[heart_predict_indices,]
nrow(heart_predict)

```






# build regression algorithm with fitting dataset:

```{r}
model <- glm(diagnosis ~ ., data=heart_fit, family="binomial")
#summary(model)
```


# see how good log reg is with fitted data:

```{r}
# build algorithm:
reg_log <- predict(model, heart_fit, type = 'response')
class_pred <- ifelse(reg_log > 0.5, 1, 0)

# get confusion matrix
confusion_matrix <- table(Predicted = class_pred, Actual = heart_fit$diagnosis)
print(confusion_matrix)
# calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(accuracy)
```


# get accuracy statistic
# see how good log reg predicts new data

```{r}
# build algorithm:
reg_log <- predict(model, heart_predict, type = 'response')
class_pred <- ifelse(reg_log > 0.5, 1, 0)

# get confusion matrix
confusion_matrix <- table(Predicted = class_pred, Actual = heart_predict$diagnosis)
print(confusion_matrix)
# calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(accuracy)
```

# add missing data (use methods that have been built before)

```{r}
## therefore include function that adds NAs in dataset:
### Define Loop functions (for multiple simulations): ----
replace_values <- function(data, cols, amount, replacement = NA, levels_amount = NULL) {


    # Verify inputs ----
    testthat::expect_true(is.data.frame(data), info = "Data should be a dataframe.") # check that data is a data frame.
    testthat::expect_true(all(cols %in% names(data)), info = "All column names should be present in the data.") # check that columns is/are all variables in the data frame.
    testthat::expect_true(length(cols) == length(amount), info = "Number of columns and percentages have to match.") # check if columns and percentages are of the same length.
    testthat::expect_true(all(is.numeric(amount) & (amount >= 0) ), info = "All amounts should be numeric values.") # check that percentages are all numbers between 0 and 1, or higher (than treat as number of to be replaced values.
    testthat::expect_true(is.character(replacement) | is.numeric(replacement) | is.logical(replacement) | is.na(replacement), info = "Replacement value should be of a valid data type (character, nummeric, logical or NA).") # check that replacement is a valid data type.
    testthat::expect_true(is.null(levels_amount) | is.list(levels_amount), info = "levels_amount should be a list") # check that levels_amount is a list if it's not NULL.
    if (!is.null(levels_amount)) {
      testthat::expect_true(all(names(levels_amount) %in% cols), info = "All names in levels_amount have to correspond to a column in the data.") # check if all elements in levels_amount correspond to a column in the data frame.
      for (col in names(levels_amount)) {
        if (is.factor(data[[col]])) {
          testthat::expect_true(all(names(levels_amount[[col]]) %in% levels(data[[col]])), info = "All levels in levels_amount have to be present in the corresponding column of the data.") # check if all sub-elements in each element are valid levels in the corresponding factor variable.

        }

        else if (is.character(data[[col]])) {
          testthat::expect_true(all(names(levels_amount[[col]]) %in% unique(data[[col]])), info = "All levels in levels_amount have to be present in the corresponding column of the data.")

        }

      }

    }
    # main: ----

    # loop function over all columns that are inserted in form of a vector and corresponding percentages:
    for (i in seq_along(cols)) {

      col <- cols[i]
      perc <- amount[i]


      # Check if the specific column is a factor and has levels_amount defined:
      if (is.factor(data[[col]]) || is.character(data[[col]]) && !is.null(levels_amount) && col %in% names(levels_amount)) {

        # if all these conditions apply for the column, code is executed:
        # get list of different replacement percentages of levels in column:
        lev_amount <- levels_amount[[col]]

        # loop over levels in current column:
        for (cat in names(lev_amount)) {

          # get replacement percentage for current category:
          replace_perc <- lev_amount[[cat]]

          # get rows in column for current category:
          rows <- which(data[[col]] == cat)

          # Calculate how many values should be replaced in category:
          num_replace <-  ifelse(replace_perc <= 1, round(replace_perc * length(rows), 0), replace_perc)

          # Use sample to replace specified percentage of category with replacement input:
          replace_rows <- sample(rows, size = num_replace[1],  replace = FALSE)
          data[replace_rows, col] <- replacement

        }

      } else {

        # Calculate how many values should be replaced:
        num_values <- nrow(data)
        num_replace <-  ifelse(amount <= 1, round(amount * num_values, 0), amount)

        # Use sample to replace specified percentage with replacement input:
        replace_rows <- sample(1:num_values,size = num_replace[1],  replace = FALSE)
        data[replace_rows, col] <- replacement

      }

    }

    # Output: ----

    return(data) # as data frame.

  } # replace_values().
```

```{r}
# Preparations:
pc <- 0.3
# get all names of variables but exclude criterion: 
column_names_heart <- colnames(heart_fit)
column_names_heart <- column_names_heart[column_names_heart != 'diagnosis']


# in total xx% should be NAs in the end, split equally over all variables: 
num_columns <- length(column_names_heart)
pc_per_row_20 <- pc

# create vector with that pc for number of rows:
pc_vector_20 <- rep(pc_per_row_20, times = num_columns)

# set seed for being able to replicate results:
set.seed(139)

# apply replace_values function and thereby create new data frame:
heart_NA_20 <- replace_values(data = heart_predict, cols = column_names_heart, amount = pc_vector_20)

```



```{r}
# first alternative: LISTWISE DELETION

# predict NA data with old model
reg_log_20_NA <- predict(model, heart_NA_20, type = 'response')
class_pred_20_NA <- ifelse(reg_log_20_NA > 0.5, 1, 0)
class_pred_20_NA
# get confusion matrix
confusion_matrix_20_NA <- table(Predicted = class_pred_20_NA, Actual = heart_predict$diagnosis)
print(confusion_matrix_20_NA)
# calculate accuracy
accuracy_20_NA <- sum(diag(confusion_matrix_20_NA)) / sum(confusion_matrix_20_NA)
print(accuracy_20_NA) # so gehts nicht einfach, weill dann einfach nur alle NAs rausgeschmissen und nicht mehr vorhergesagt werden!

```

# median imputation technique


```{r}

# Replace NA values with the median of each column
heart_median_imputed <- as.data.frame(lapply(heart_NA_20, function(x) {
  if(is.numeric(x)) {
    return(ifelse(is.na(x), median(x, na.rm = TRUE), x))
  } else {
    return(x)
  }
}))

# Replace NA values in categorical columns with the mode
heart_median_imputed <- as.data.frame(lapply(heart_median_imputed, function(x) {
  if(is.factor(x) || is.character(x)) {
    mode_value <- names(sort(table(x), decreasing = TRUE))[1]
    return(ifelse(is.na(x), mode_value, x))
  } else {
    return(x)
  }
}))

# predict NA data with median imputed model
reg_log_20_NA <- predict(model, heart_median_imputed, type = 'response')
class_pred_20_NA <- ifelse(reg_log_20_NA > 0.5, 1, 0)

# get confusion matrix
confusion_matrix_20_NA <- table(Predicted = class_pred_20_NA, Actual = heart_predict$diagnosis)
print(confusion_matrix_20_NA)
# calculate accuracy
accuracy_20_NA <- sum(diag(confusion_matrix_20_NA)) / sum(confusion_matrix_20_NA)
print(accuracy_20_NA) 
#heart_median_imputed
```



# somehow handle NA data to be able to use log reg on the new datasets

# imputation
```{r}
# prepare pc
pc_100 <- pc*100 
pc_100
```

# prepare data (so that criterion is not used in imputation)
```{r}
original_diagnosis <- heart_NA_20$diagnosis
heart_NA_20_without_crit <- heart_NA_20
heart_NA_20_without_crit$diagnosis <- NULL
```


```{r}
require(mice)
#str(heart_NA_20) 
# convert character variables into factor
heart_NA_20_without_crit <- data.frame(lapply(heart_NA_20_without_crit, function(x) if(is.character(x)) as.factor(x) else x))
#str(heart_NA_20) 
```


```{r}

# Multiple Imputation with the mice package
# 1. Imputation
imp.data <- mice (data = heart_NA_20_without_crit, m = pc_100, maxit = 10, seed = 123, print=FALSE)
# Which methods were used?
#imp.data
# Checking if convergence was achieved
#plot(imp.data)
# 2. Regression for each imputed data set
#reg.fit.mi <- with(imp.data, glm(diagnosis ~age+sex+cp+trestbps+chol+fbs+restecg+thalach+exang+oldpeak+slope+ca+thal,   family = "binomial"))
#reg.fit.mi
# 3. Pooling the results
#pool.fit <- pool(reg.fit.mi)
#summary(pool.fit)

```


# get accuracy performance!

```{r}
# Apply the logistic regression model to each imputed data set
predictions <- lapply(1:pc_100, function(m) {
  dataset <- complete(imp.data, m)
  dataset$diagnosis <- original_diagnosis
  predict(glm(diagnosis ~., family = "binomial", data = dataset), type = "response")
})

# Average the predictions
avg_predictions <- Reduce("+", predictions) / pc_100

# Create binary classification
class_pred_imputed <- ifelse(avg_predictions > 0.5, 1, 0)

# create confusion matrix
confusion_matrix_imputed <- table(Predicted = class_pred_imputed, Actual = heart_predict$diagnosis)
print(confusion_matrix_imputed)

# Calculate accuracy
accuracy_imputed <- sum(diag(confusion_matrix_imputed)) / sum(confusion_matrix_imputed)
print(accuracy_imputed)

```


# compare performance of best log reg NA handling method on different NA percentages with the performance of other algorithms

```{r}
# without NAs, baseline performance for train data:

# build best tree. (standard function)
heart.fft <- FFTrees(formula = diagnosis ~.,
                           data = heart_fit,
                           main = "Heart Disease",
                           goal = "acc",
                           goal.chase = "acc",
                           goal.threshold = "acc",
                           decision.labels = c("Healthy", "Diseased"))


# Report on NA cases: 
tree_level_outcome_vars <- c(1:3, 7:12, 25:30)
heart.fft$trees$level_stats$train[ , tree_level_outcome_vars]
  
plot(heart.fft, data = "train")
```



```{r}
# without NAs, baseline performance for test data:
# build best tree. (standard function)
heart.fft <- FFTrees(formula = diagnosis ~.,
                           data = heart_fit,
                           data.test =  heart_predict,
                           main = "Heart Disease",
                           goal = "acc",
                           goal.chase = "acc",
                           goal.threshold = "acc",
                           decision.labels = c("Healthy", "Diseased"))


# Report on NA cases: 
tree_level_outcome_vars <- c(1:3, 7:12, 25:30)
heart.fft$trees$level_stats$test[ , tree_level_outcome_vars]
  
plot(heart.fft, data = "test")
```


```{r}

# with 20% NAs:
# build best tree. 20% NAs (standard function)
heart.fft_NA_20 <- FFTrees(formula = diagnosis ~.,
                           data = heart_fit,
                           data.test =  heart_NA_20,
                           goal = "acc",
                           goal.chase = "acc",
                           goal.threshold = "acc",
                           main = "Heart Disease",
                           decision.labels = c("Healthy", "Diseased"))


# Report on NA cases: 
tree_level_outcome_vars <- c(1:3, 7:12, 25:30)
heart.fft_NA_20$trees$level_stats$train[ , tree_level_outcome_vars]
  
plot(heart.fft_NA_20, data = "test")
```


```{r}

# Creating a single imputed dataset
long_data <- complete(imp.data, "long")
# Calculate the mean of imputed values for each variable
aggregated_data <- aggregate(. ~ .imp + .id, data = long_data, FUN = mean)

# remove .imp and .id columns to get the final dataset
final_imputed_data <- aggregated_data[, !(names(aggregated_data) %in% c(".imp", ".id"))]

# change heart_fit to same types variables as imputed data
heart_fit_types_dif <- data.frame(lapply(heart_fit, function(x) if(is.character(x)) as.numeric(x) else x))

# add criterion to imputed data
final_imputed_data$diagnosis <- original_diagnosis

# convert  criterion back into logical
final_imputed_data$diagnosis <- as.logical(final_imputed_data$diagnosis == 1)
heart_fit_types_dif$diagnosis <- as.logical(heart_fit_types_dif$diagnosis == 1)


# Using the final imputed data to train FFTrees
heart.fft <- FFTrees(formula = diagnosis ~ .,
                     data = heart_fit_types_dif,
                     data.test = final_imputed_data,
                     goal = "acc",
                     goal.chase = "acc",
                     goal.threshold = "acc",
                     main = "Heart Disease",
                     decision.labels = c("Healthy", "Diseased"))

print(heart.fft)
plot(heart.fft, data = "test")
str(final_imputed_data)
str(heart_fit_types_dif)

```





